{"cells":[{"cell_type":"code","execution_count":16,"id":"97b654d8-b162-44d5-ab57-725d9e10695e","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":null,"execution_start_time":null,"livy_statement_state":null,"normalized_state":"waiting","parent_msg_id":"95e41a5e-f0c2-463a-8736-fcb4a50f3b09","queued_time":"2024-11-10T19:46:43.2195263Z","session_id":null,"session_start_time":null,"spark_pool":null,"state":"waiting","statement_id":null,"statement_ids":null},"text/plain":["StatementMeta(, , , Waiting, , Waiting)"]},"metadata":{},"output_type":"display_data"}],"source":["# df = spark.sql(\"SELECT * FROM InFreGen.task LIMIT 1000\")\n","# display(df)\n","# # 删除所有给定taskID的pic\n","# spark.sql(\"DELETE FROM InFreGen.pic WHERE taskID = {}\".format(taskID))\n","# taskID = 19"]},{"cell_type":"code","execution_count":17,"id":"8ff2a0b8-3a27-4094-99da-59a1b14d2448","metadata":{"collapsed":false,"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":null,"execution_start_time":null,"livy_statement_state":null,"normalized_state":"waiting","parent_msg_id":"e4c29793-9b3e-43bc-a941-bc8aa25af9ae","queued_time":"2024-11-10T19:46:43.220363Z","session_id":null,"session_start_time":null,"spark_pool":null,"state":"waiting","statement_id":null,"statement_ids":null},"text/plain":["StatementMeta(, , , Waiting, , Waiting)"]},"metadata":{},"output_type":"display_data"}],"source":["from enum import Enum\n","\n","import requests\n","from pyspark.sql import functions as F\n","import json\n","\n","ACCESS_KEY = 'YOUR KEY HERE'\n","\n","\n","class Orientation(Enum):\n","    UNKNOWN = 0\n","    LANDSCAPE = 1\n","    PORTRAIT = 2\n","    SQUARISH = 3\n","\n","    def get_lower_value(self):\n","        if self == Orientation.UNKNOWN:\n","            return \"\"\n","        return self.name.lower()\n","\n","\n","class UnsplashClient:\n","    def __init__(self, access_key):\n","        self.access_key = access_key\n","        self.base_url = 'https://api.unsplash.com/'\n","        self.headers = {\n","            'Accept-Version': 'v1',\n","            'Authorization': f'Client-ID {self.access_key}'\n","        }\n","\n","    def get_random_image(self, query: str = None, orientation: Orientation = Orientation.UNKNOWN):\n","        url = self.base_url + 'photos/random'\n","        params = {\n","            'query': query,\n","            'orientation': orientation.get_lower_value()\n","        }\n","        response = requests.get(url, headers=self.headers, params=params)\n","        return response.json() if response.status_code == 200 else None\n","\n","    def search_images(\n","            self, query: str,\n","            page: int = 1,\n","            per_page: int = 30,\n","            orientation: Orientation = Orientation.UNKNOWN\n","    ):\n","        url = self.base_url + 'search/photos'\n","        params = {\n","            'query': query,\n","            'page': page,\n","            'per_page': per_page,\n","        }\n","        if orientation != Orientation.UNKNOWN:\n","            params['orientation'] = orientation.get_lower_value()\n","        response = requests.get(url, headers=self.headers, params=params)\n","        print(response.status_code)\n","        # 400 error \n","        return response.json() if response.status_code == 200 else None\n","\n","    # 你可以在这里添加更多的方法，比如获取特定图片的详细信息等等。\n","\n","client = UnsplashClient(ACCESS_KEY)\n"]},{"cell_type":"code","execution_count":18,"id":"6aa029c7-a5e1-4cc7-97ba-4fc347343359","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":null,"execution_start_time":null,"livy_statement_state":null,"normalized_state":"waiting","parent_msg_id":"0c639b21-a63e-4dee-8366-01fe2e81a591","queued_time":"2024-11-10T19:46:43.2210254Z","session_id":null,"session_start_time":null,"spark_pool":null,"state":"waiting","statement_id":null,"statement_ids":null},"text/plain":["StatementMeta(, , , Waiting, , Waiting)"]},"metadata":{},"output_type":"display_data"}],"source":["import random\n","# # Query to find rows where 'url' is an empty string\n","# result_df = spark.sql(\"SELECT * FROM InFreGen.task WHERE State = 0\")\n","\n","# # Show the result\n","# display(result_df)\n","# # Select the 'userInput' column from the result_df DataFrame\n","# user_input_df = result_df.select(\"userInput\")\n","\n","# # Collect the userInput values into a list\n","# user_input_values = user_input_df.collect()\n","\n","# # Print the collected userInput values\n","# for row in user_input_values:\n","#     print(row['userInput'])\n","\n","task_row = spark.sql(\"SELECT * FROM InFreGen.task WHERE taskID = {}\".format(taskID)).collect()[0]\n","print(task_row)\n","# Row(taskID=12, userInput='I want to do medical image cancer detection', keyword=None, num=8, resolution='1024x1024', sizeChoice='small', State=0)\n","# use the client to search the pic by the task_row['keyword']\n","def searchPerTask(task_row) -> list[dict]:\n","    keywords = task_row['keyword'].split(',')\n","    print(\"keyword:\", keywords)\n","    pic_table = []\n","    count = 0\n","    while count < task_row['num']:\n","        keyword = random.choice(keywords)\n","\n","        print(\"Chosen keyword: \" + keyword)\n","\n","        search_results = client.search_images(query=keyword, page=1, per_page=30, orientation=Orientation.SQUARISH)\n","        try:\n","            image_urls = random.choice(search_results['results'])['urls']\n","        except:\n","            continue\n","        # add a new row to the pic table\n","        new_row = {\n","            'taskID': task_row['taskID'],\n","            'picID': count,\n","            'Resolution': task_row['resolution'],\n","            'sizeChoice': task_row['sizeChoice'],\n","            'operations': \"\",\n","            'operationsReturn': \"\",\n","            'url': json.dumps(image_urls),\n","            'State': 0,\n","            'originalPicPath': \"\",\n","            'curPicPath': \"\",\n","            'keywords': keyword,\n","            'finalPicPath': \"\",\n","        }\n","        pic_table.append(new_row)\n","        count += 1\n","    return pic_table\n","    \n","# inject the pic table with the pic_df\n","from pyspark.sql.types import IntegerType\n","\n","pic_df = spark.createDataFrame(searchPerTask(task_row))\n","pic_df = pic_df.withColumn(\"taskID\", pic_df[\"taskID\"].cast(IntegerType()))\n","pic_df = pic_df.withColumn(\"picID\", pic_df[\"picID\"].cast(IntegerType()))\n","pic_df = pic_df.withColumn(\"State\", pic_df[\"State\"].cast(IntegerType()))\n","pic_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"InFreGen.pic\")"]},{"cell_type":"code","execution_count":19,"id":"85376c12-771b-4df6-af38-87b700aee4b4","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":null,"execution_start_time":null,"livy_statement_state":null,"normalized_state":"waiting","parent_msg_id":"81a8198f-b157-45bb-bfeb-030cd448aca0","queued_time":"2024-11-10T19:46:43.2216427Z","session_id":null,"session_start_time":null,"spark_pool":null,"state":"waiting","statement_id":null,"statement_ids":null},"text/plain":["StatementMeta(, , , Waiting, , Waiting)"]},"metadata":{},"output_type":"display_data"}],"source":["# df = spark.sql(\"SELECT * FROM InFreGen.pic LIMIT 1000\")\n","# display(df)"]},{"cell_type":"code","execution_count":20,"id":"742f0a74-4ab1-484b-9398-02d57f817614","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":null,"execution_start_time":null,"livy_statement_state":null,"normalized_state":"waiting","parent_msg_id":"eb7efb14-1083-4282-aa49-bbe9330ca0b6","queued_time":"2024-11-10T19:46:43.2223127Z","session_id":null,"session_start_time":null,"spark_pool":null,"state":"waiting","statement_id":null,"statement_ids":null},"text/plain":["StatementMeta(, , , Waiting, , Waiting)"]},"metadata":{},"output_type":"display_data"}],"source":["# # 使用示例\n","\n","# client = UnsplashClient(ACCESS_KEY)\n","\n","# # 创建 Delta 表 pic\n","# # DeltaTable.create(spark) \\\n","# #   .tableName(\"pic\") \\\n","# #   .addColumn(\"taskID\", \"INT\") \\\n","# #   .addColumn(\"picID\", \"INT\") \\\n","# #   .addColumn(\"Resolution\", \"STRING\") \\\n","# #   .addColumn(\"sizeChoice\", \"String\") \\\n","# #   .addColumn(\"operations\", \"STRING\") \\\n","# #   .addColumn(\"operationsReturn\", \"STRING\") \\\n","# #   .addColumn(\"url\", \"STRING\") \\\n","# #   .addColumn(\"originalPicPath\", \"STRING\") \\\n","# #   .addColumn(\"State\", \"INT\") \\\n","# #   .addColumn(\"curPicPath\", \"STRING\") \\\n","# #   .addColumn(\"finalPicPath\", \"STRING\") \\\n","# #   .execute()\n","# import json\n","# from pyspark.sql import functions as F\n","# from pyspark.sql import Row\n","\n","# # Query to find rows where 'url' is an empty string\n","# # Collect all necessary values\n","# task_values = result_df.collect()\n","\n","# # Print the collected userInput values and create pic_table\n","# pic_table = []\n","# for row in task_values:\n","#     print(row['userInput'])\n","#     if row['userInput'] == \"I want to do instance segmentation\":\n","#         keyword = \"Vase\"\n","#     elif row['userInput'] == \"I want to do semantic segmentation\":\n","#         keyword = \"Car\"\n","#     elif row['userInput'] == \"I want to do text to image\":\n","#         keyword = \"Flower\"\n","#     elif row['userInput'] == \"I want to do image captioning\":\n","#         keyword = \"Dog\"\n","#     elif row['userInput'] == \"I want to do Cat and Dog Classification\":\n","#         keyword = \"Cat\"\n","#     elif row['userInput'] == \"I want to do Animal Classification\":\n","#         keyword = \"Dog\" \n","#     else:\n","#         keyword = \"Cat\"     \n","#     search_results = client.search_images(query=keyword, page=1, per_page=30, orientation=Orientation.SQUARISH)\n","\n","#     if search_results and 'results' in search_results and len(search_results['results']) > 0:\n","#         for i in range(row['num']):\n","#             # create new row as a dictionary\n","#             new_row = {\n","#                 'taskID': row['taskID'],\n","#                 'picID': i,\n","#                 'Resolution': \"1024x1024\",\n","#                 'sizeChoice': \"small\",\n","#                 'operations': \"\",\n","#                 'operationsReturn': \"\",\n","#                 'url': \"\",\n","#                 'originalPicPath': \"\",\n","#                 'State': 1,\n","#                 'curPicPath': \"\",\n","#                 'finalPicPath': \"\"\n","#             }\n","#             image_urls = search_results['results'][i]['urls']\n","\n","#             # Serialize image URLs data as a dictionary\n","#             serialized_url_data = {\n","#                 'raw': image_urls.get('raw'),\n","#                 'full': image_urls.get('full'),\n","#                 'regular': image_urls.get('regular'),\n","#                 'small': image_urls.get('small'),\n","#                 'thumb': image_urls.get('thumb'),\n","#                 'small_s3': image_urls.get('small_s3')\n","#             }\n","#             # Convert the dictionary into a string representation\n","#             new_row['url'] = json.dumps(serialized_url_data)\n","#             pic_table.append(new_row)\n","\n","# # Convert pic_table to a DataFrame\n","# pic_df = spark.createDataFrame(pic_table)\n","# display(pic_df)"]},{"cell_type":"code","execution_count":21,"id":"eb95b18f-3b93-4ae1-80f7-849d4a55fe91","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":null,"execution_start_time":null,"livy_statement_state":null,"normalized_state":"waiting","parent_msg_id":"cb61e463-c3bc-4989-9d8f-126dc9b5fe5a","queued_time":"2024-11-10T19:46:43.2230052Z","session_id":null,"session_start_time":null,"spark_pool":null,"state":"waiting","statement_id":null,"statement_ids":null},"text/plain":["StatementMeta(, , , Waiting, , Waiting)"]},"metadata":{},"output_type":"display_data"}],"source":["# from pyspark.sql import functions as F\n","# from delta.tables import DeltaTable\n","# from pyspark.sql.types import StringType\n","# from pyspark.sql.types import IntegerType\n","# delta_table = DeltaTable.forName(spark, \"InFreGen.pic\")\n","\n","# # # Perform the MERGE operation to update rows based on taskID and picID\n","# # delta_table.alias(\"target\") \\\n","# #     .merge(\n","# #         result_df.alias(\"source\"),\n","# #         \"target.taskID = source.taskID AND target.picID = source.picID\"  # Condition for matching rows\n","# #     ) \\\n","# #     .whenMatchedUpdate(\n","# #         condition=\"target.url != source.url\",  # Only update if the URL is different\n","# #         set={\"url\": \"source.url\"}  # Update the URL column with the new value\n","# #     ) \\\n","# #     .whenNotMatchedInsert(\n","# #         values={\"taskID\": \"source.taskID\", \"picID\": \"source.picID\", \"url\": \"source.url\"}  # Insert new row if no match\n","# #     ) \\\n","# #     .execute()\n","\n","# # # Show the updated table content (optional)\n","# # delta_table.toDF().show(truncate=False)\n","# # inject the pic table with the pic_df\n","# # inject the pic table with the pic_df\n","# pic_df = pic_df.withColumn(\"taskID\", pic_df[\"taskID\"].cast(IntegerType()))\n","# pic_df = pic_df.withColumn(\"picID\", pic_df[\"picID\"].cast(IntegerType()))\n","# pic_df = pic_df.withColumn(\"State\", pic_df[\"State\"].cast(IntegerType()))\n","# pic_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"InFreGen.pic\")\n"]},{"cell_type":"code","execution_count":22,"id":"6c4f60fd-b076-4b9f-98f8-f71a8ce5e77f","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":null,"execution_start_time":null,"livy_statement_state":null,"normalized_state":"waiting","parent_msg_id":"1621082f-b614-4684-b7c3-2cd42b2f0de0","queued_time":"2024-11-10T19:46:43.223693Z","session_id":null,"session_start_time":null,"spark_pool":null,"state":"waiting","statement_id":null,"statement_ids":null},"text/plain":["StatementMeta(, , , Waiting, , Waiting)"]},"metadata":{},"output_type":"display_data"}],"source":["# df = spark.sql(\"SELECT * FROM InFreGen.pic LIMIT 1000\")\n","# display(df)"]}],"metadata":{"dependencies":{"lakehouse":{"default_lakehouse":"89188849-0b09-4102-bcab-2cfcf5509b4b","default_lakehouse_name":"InFreGen","default_lakehouse_workspace_id":"a136438a-0d8b-4308-9f2f-c74ea9668405"}},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"synapse_widget":{"state":{},"version":"0.1"},"widgets":{}},"nbformat":4,"nbformat_minor":5}
