{"cells":[{"cell_type":"markdown","id":"1a93b500-6713-4ed0-bc50-9ec98d6b97e2","metadata":{"id":"Wgj3tUoobAlj"},"source":["# Combining Grounding DINO with Segment Anything (SAM) for text-based mask generation\n","\n","In this notebook, we're going to combine 2 very cool models - [Grounding DINO](https://huggingface.co/docs/transformers/main/en/model_doc/grounding-dino) and [SAM](https://huggingface.co/docs/transformers/en/model_doc/sam). We'll use Grounding DINO to generate bounding boxes based on text prompts, after which we can prompt SAM to generate corresponding segmentation masks for them.\n","\n","This is based on the popular [Grounded Segment Anything](https://github.com/IDEA-Research/Grounded-Segment-Anything) project - just with fewer lines of code as the models are now available in the Transformers library. Refer to the [paper](https://arxiv.org/abs/2401.14159) for details.\n","\n","<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/grounded_sam.png\"\n","alt=\"drawing\" width=\"900\"/>\n","\n","<small> Grounded SAM overview. Taken from the <a href=\"https://github.com/IDEA-Research/Grounded-Segment-Anything\">original repository</a>. </small>\n","\n","Author of this notebook: [Eduardo Pacheco](https://huggingface.co/EduardoPacheco) - give him a follow on Hugging\n"," Face!\n","\n","## Set-up environment\n","\n","Let's start by installing ðŸ¤— Transformers from source since Grounding DINO is brand new at the time of writing."]},{"cell_type":"code","execution_count":null,"id":"82bd3166-59a8-485e-841d-825535df1cb8","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"78i_LKhJYz8M","microsoft":{"language":"python","language_group":"synapse_pyspark"},"outputId":"c3e2387a-e82d-4fd7-8bd9-5d4b90e829d6"},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-11-03T22:10:07.2414082Z","execution_start_time":"2024-11-03T22:10:07.0106975Z","livy_statement_state":"available","normalized_state":"finished","parent_msg_id":"fc4642c5-0ffa-4ba7-ae68-375fffb99965","queued_time":"2024-11-03T22:10:05.5748416Z","session_id":"8e0b499d-634c-4227-b064-12912a5bedf1","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":171,"statement_ids":[171]},"text/plain":["StatementMeta(, 8e0b499d-634c-4227-b064-12912a5bedf1, 171, Finished, Available, Finished)"]},"metadata":{},"output_type":"display_data"}],"source":["# !pip install --upgrade -q git+https://github.com/huggingface/transformers\n","# !pip install --upgrade pip\n"]},{"cell_type":"code","execution_count":null,"id":"702c07d3-d102-449d-b408-da61e7e564b0","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-11-03T22:10:07.8548852Z","execution_start_time":"2024-11-03T22:10:07.6120816Z","livy_statement_state":"available","normalized_state":"finished","parent_msg_id":"0c9bdd56-94df-4875-a2a3-301387214f9e","queued_time":"2024-11-03T22:10:05.6323461Z","session_id":"8e0b499d-634c-4227-b064-12912a5bedf1","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":172,"statement_ids":[172]},"text/plain":["StatementMeta(, 8e0b499d-634c-4227-b064-12912a5bedf1, 172, Finished, Available, Finished)"]},"metadata":{},"output_type":"display_data"}],"source":["# from notebookutils import mssparkutils\n","# mssparkutils.session.restartPython()"]},{"cell_type":"markdown","id":"9357b4c9-3f00-472f-9b07-54c19e887b9b","metadata":{"id":"7r9dNrDHy2tA"},"source":["## Imports\n","\n","Let's start by importing the required libraries."]},{"cell_type":"code","execution_count":null,"id":"defd795e-f110-4e80-a2f0-aee5450316ce","metadata":{"id":"Y2vA9eeacmIc","microsoft":{"language":"python","language_group":"synapse_pyspark"}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-11-03T22:10:08.4822879Z","execution_start_time":"2024-11-03T22:10:08.2558849Z","livy_statement_state":"available","normalized_state":"finished","parent_msg_id":"7cccd26f-adfc-414f-b41d-7e28c55bf1e2","queued_time":"2024-11-03T22:10:05.6925899Z","session_id":"8e0b499d-634c-4227-b064-12912a5bedf1","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":173,"statement_ids":[173]},"text/plain":["StatementMeta(, 8e0b499d-634c-4227-b064-12912a5bedf1, 173, Finished, Available, Finished)"]},"metadata":{},"output_type":"display_data"}],"source":["import random\n","from dataclasses import dataclass\n","from typing import Any, List, Dict, Optional, Union, Tuple\n","\n","# import cv2\n","import torch\n","import requests\n","import numpy as np\n","from PIL import Image\n","import plotly.express as px\n","import matplotlib.pyplot as plt\n","import plotly.graph_objects as go\n","from transformers import AutoModelForMaskedLM, AutoProcessor, AutoModelForZeroShotObjectDetection"]},{"cell_type":"markdown","id":"7291281a-fbd3-4400-b8e2-9784a78bd80d","metadata":{"id":"A1NxJzCNrnjH"},"source":["## Result Utils\n","\n","We'll store the detection results of Grounding DINO in a dedicated Python dataclass."]},{"cell_type":"code","execution_count":null,"id":"b8559d54-84b6-406c-80f7-46704241b50d","metadata":{"id":"ZgeXiUwIrpqJ","microsoft":{"language":"python","language_group":"synapse_pyspark"}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-11-03T22:10:09.1112614Z","execution_start_time":"2024-11-03T22:10:08.8888395Z","livy_statement_state":"available","normalized_state":"finished","parent_msg_id":"7b6b0123-ff3a-4083-a3f5-bea0409d7eb2","queued_time":"2024-11-03T22:10:05.7770784Z","session_id":"8e0b499d-634c-4227-b064-12912a5bedf1","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":174,"statement_ids":[174]},"text/plain":["StatementMeta(, 8e0b499d-634c-4227-b064-12912a5bedf1, 174, Finished, Available, Finished)"]},"metadata":{},"output_type":"display_data"}],"source":["@dataclass\n","class BoundingBox:\n","    xmin: int\n","    ymin: int\n","    xmax: int\n","    ymax: int\n","\n","    @property\n","    def xyxy(self) -> List[float]:\n","        return [self.xmin, self.ymin, self.xmax, self.ymax]\n","\n","@dataclass\n","class DetectionResult:\n","    score: float\n","    label: str\n","    box: BoundingBox\n","    mask: Optional[np.array] = None\n","\n","    @classmethod\n","    def from_dict(cls, detection_dict: Dict) -> 'DetectionResult':\n","        return cls(score=detection_dict['score'],\n","                   label=detection_dict['label'],\n","                   box=BoundingBox(xmin=detection_dict['box']['xmin'],\n","                                   ymin=detection_dict['box']['ymin'],\n","                                   xmax=detection_dict['box']['xmax'],\n","                                   ymax=detection_dict['box']['ymax']))"]},{"cell_type":"markdown","id":"bd6fe61d-c3da-40b6-b6df-3ba8a8dd59ed","metadata":{"id":"uCzSUQL5lAvE"},"source":["## Plot Utils\n","\n","Below, some utility functions are defined as we'll draw the detection results of Grounding DINO on top of the image."]},{"cell_type":"code","execution_count":null,"id":"8d824a81-b55c-4a9a-b05e-60af9084800b","metadata":{"id":"Zah3Esewo4P6","microsoft":{"language":"python","language_group":"synapse_pyspark"}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-11-03T22:10:09.7437104Z","execution_start_time":"2024-11-03T22:10:09.525106Z","livy_statement_state":"available","normalized_state":"finished","parent_msg_id":"2c506625-617d-4e9e-a3c5-3ef8ca0be71a","queued_time":"2024-11-03T22:10:05.9104191Z","session_id":"8e0b499d-634c-4227-b064-12912a5bedf1","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":175,"statement_ids":[175]},"text/plain":["StatementMeta(, 8e0b499d-634c-4227-b064-12912a5bedf1, 175, Finished, Available, Finished)"]},"metadata":{},"output_type":"display_data"}],"source":["def annotate(image: Union[Image.Image, np.ndarray], detection_results: List[DetectionResult]) -> np.ndarray:\n","    # Convert to PIL Image if input is ndarray\n","    if isinstance(image, np.ndarray):\n","        image = Image.fromarray(image)\n","    \n","    # Create a drawing context\n","    draw = ImageDraw.Draw(image)\n","\n","    # Iterate over detections and add bounding boxes and masks\n","    for detection in detection_results:\n","        label = detection.label\n","        score = detection.score\n","        box = detection.box\n","        mask = detection.mask\n","\n","        # Sample a random color for each detection\n","        color = tuple(random.randint(0, 255) for _ in range(3))\n","\n","        # Draw bounding box\n","        draw.rectangle([box.xmin, box.ymin, box.xmax, box.ymax], outline=color, width=2)\n","        draw.text((box.xmin, box.ymin - 10), f'{label}: {score:.2f}', fill=color)\n","\n","        # If mask is available, apply it using matplotlib to display the contours\n","        if mask is not None:\n","            mask_image = Image.fromarray((mask * 255).astype(np.uint8))\n","            plt.imshow(image)\n","            plt.imshow(mask_image, alpha=0.3, cmap=\"jet\")\n","            plt.axis('off')\n","            plt.show()\n","    \n","    # Convert the image back to numpy array if needed\n","    return np.array(image)\n","def plot_detections(\n","    image: Union[Image.Image, np.ndarray],\n","    detections: List[DetectionResult],\n","    save_name: Optional[str] = None\n",") -> None:\n","    annotated_image = annotate(image, detections)\n","    plt.imshow(annotated_image)\n","    plt.axis('off')\n","    if save_name:\n","        plt.savefig(save_name, bbox_inches='tight')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"id":"d55fa6ac-5eaa-4161-b8cc-16aa9cdfdb05","metadata":{"id":"Wzjx3MLPjyW8","microsoft":{"language":"python","language_group":"synapse_pyspark"}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-11-03T22:10:10.3776008Z","execution_start_time":"2024-11-03T22:10:10.1472476Z","livy_statement_state":"available","normalized_state":"finished","parent_msg_id":"88ddc29c-1008-49df-b94f-9cd67c716803","queued_time":"2024-11-03T22:10:06.0629083Z","session_id":"8e0b499d-634c-4227-b064-12912a5bedf1","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":176,"statement_ids":[176]},"text/plain":["StatementMeta(, 8e0b499d-634c-4227-b064-12912a5bedf1, 176, Finished, Available, Finished)"]},"metadata":{},"output_type":"display_data"}],"source":["def random_named_css_colors(num_colors: int) -> List[str]:\n","    \"\"\"\n","    Returns a list of randomly selected named CSS colors.\n","\n","    Args:\n","    - num_colors (int): Number of random colors to generate.\n","\n","    Returns:\n","    - list: List of randomly selected named CSS colors.\n","    \"\"\"\n","    # List of named CSS colors\n","    named_css_colors = [\n","        'aliceblue', 'antiquewhite', 'aqua', 'aquamarine', 'azure', 'beige', 'bisque', 'black', 'blanchedalmond',\n","        'blue', 'blueviolet', 'brown', 'burlywood', 'cadetblue', 'chartreuse', 'chocolate', 'coral', 'cornflowerblue',\n","        'cornsilk', 'crimson', 'cyan', 'darkblue', 'darkcyan', 'darkgoldenrod', 'darkgray', 'darkgreen', 'darkgrey',\n","        'darkkhaki', 'darkmagenta', 'darkolivegreen', 'darkorange', 'darkorchid', 'darkred', 'darksalmon', 'darkseagreen',\n","        'darkslateblue', 'darkslategray', 'darkslategrey', 'darkturquoise', 'darkviolet', 'deeppink', 'deepskyblue',\n","        'dimgray', 'dimgrey', 'dodgerblue', 'firebrick', 'floralwhite', 'forestgreen', 'fuchsia', 'gainsboro', 'ghostwhite',\n","        'gold', 'goldenrod', 'gray', 'green', 'greenyellow', 'grey', 'honeydew', 'hotpink', 'indianred', 'indigo', 'ivory',\n","        'khaki', 'lavender', 'lavenderblush', 'lawngreen', 'lemonchiffon', 'lightblue', 'lightcoral', 'lightcyan', 'lightgoldenrodyellow',\n","        'lightgray', 'lightgreen', 'lightgrey', 'lightpink', 'lightsalmon', 'lightseagreen', 'lightskyblue', 'lightslategray',\n","        'lightslategrey', 'lightsteelblue', 'lightyellow', 'lime', 'limegreen', 'linen', 'magenta', 'maroon', 'mediumaquamarine',\n","        'mediumblue', 'mediumorchid', 'mediumpurple', 'mediumseagreen', 'mediumslateblue', 'mediumspringgreen', 'mediumturquoise',\n","        'mediumvioletred', 'midnightblue', 'mintcream', 'mistyrose', 'moccasin', 'navajowhite', 'navy', 'oldlace', 'olive',\n","        'olivedrab', 'orange', 'orangered', 'orchid', 'palegoldenrod', 'palegreen', 'paleturquoise', 'palevioletred', 'papayawhip',\n","        'peachpuff', 'peru', 'pink', 'plum', 'powderblue', 'purple', 'rebeccapurple', 'red', 'rosybrown', 'royalblue', 'saddlebrown',\n","        'salmon', 'sandybrown', 'seagreen', 'seashell', 'sienna', 'silver', 'skyblue', 'slateblue', 'slategray', 'slategrey',\n","        'snow', 'springgreen', 'steelblue', 'tan', 'teal', 'thistle', 'tomato', 'turquoise', 'violet', 'wheat', 'white',\n","        'whitesmoke', 'yellow', 'yellowgreen'\n","    ]\n","\n","    # Sample random named CSS colors\n","    return random.sample(named_css_colors, min(num_colors, len(named_css_colors)))\n","\n","def plot_detections_plotly(\n","    image: np.ndarray,\n","    detections: List[DetectionResult],\n","    class_colors: Optional[Dict[str, str]] = None\n",") -> None:\n","    # If class_colors is not provided, generate random colors for each class\n","    if class_colors is None:\n","        num_detections = len(detections)\n","        colors = random_named_css_colors(num_detections)\n","        class_colors = {}\n","        for i in range(num_detections):\n","            class_colors[i] = colors[i]\n","\n","\n","    fig = px.imshow(image)\n","\n","    # Add bounding boxes\n","    shapes = []\n","    annotations = []\n","    for idx, detection in enumerate(detections):\n","        label = detection.label\n","        box = detection.box\n","        score = detection.score\n","        mask = detection.mask\n","\n","        polygon = mask_to_polygon(mask)\n","\n","        fig.add_trace(go.Scatter(\n","            x=[point[0] for point in polygon] + [polygon[0][0]],\n","            y=[point[1] for point in polygon] + [polygon[0][1]],\n","            mode='lines',\n","            line=dict(color=class_colors[idx], width=2),\n","            fill='toself',\n","            name=f\"{label}: {score:.2f}\"\n","        ))\n","\n","        xmin, ymin, xmax, ymax = box.xyxy\n","        shape = [\n","            dict(\n","                type=\"rect\",\n","                xref=\"x\", yref=\"y\",\n","                x0=xmin, y0=ymin,\n","                x1=xmax, y1=ymax,\n","                line=dict(color=class_colors[idx])\n","            )\n","        ]\n","        annotation = [\n","            dict(\n","                x=(xmin+xmax) // 2, y=(ymin+ymax) // 2,\n","                xref=\"x\", yref=\"y\",\n","                text=f\"{label}: {score:.2f}\",\n","            )\n","        ]\n","\n","        shapes.append(shape)\n","        annotations.append(annotation)\n","\n","    # Update layout\n","    button_shapes = [dict(label=\"None\",method=\"relayout\",args=[\"shapes\", []])]\n","    button_shapes = button_shapes + [\n","        dict(label=f\"Detection {idx+1}\",method=\"relayout\",args=[\"shapes\", shape]) for idx, shape in enumerate(shapes)\n","    ]\n","    button_shapes = button_shapes + [dict(label=\"All\", method=\"relayout\", args=[\"shapes\", sum(shapes, [])])]\n","\n","    fig.update_layout(\n","        xaxis=dict(visible=False),\n","        yaxis=dict(visible=False),\n","        # margin=dict(l=0, r=0, t=0, b=0),\n","        showlegend=True,\n","        updatemenus=[\n","            dict(\n","                type=\"buttons\",\n","                direction=\"up\",\n","                buttons=button_shapes\n","            )\n","        ],\n","        legend=dict(\n","            orientation=\"h\",\n","            yanchor=\"bottom\",\n","            y=1.02,\n","            xanchor=\"right\",\n","            x=1\n","        )\n","    )\n","\n","    # Show plot\n","    fig.show()\n"]},{"cell_type":"markdown","id":"23ab8e6a-2f58-4ead-9f8e-d9419ea48783","metadata":{"id":"A856bC-Nha45"},"source":["## Utils"]},{"cell_type":"code","execution_count":null,"id":"f3c373b3-03df-40a8-8e21-defcd5e38fe2","metadata":{"id":"vIqTxb5LhcjU","microsoft":{"language":"python","language_group":"synapse_pyspark"}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-11-03T22:10:11.0056365Z","execution_start_time":"2024-11-03T22:10:10.7663547Z","livy_statement_state":"available","normalized_state":"finished","parent_msg_id":"d3ef2a5f-dcfc-4f37-b561-910099aade26","queued_time":"2024-11-03T22:10:06.1688251Z","session_id":"8e0b499d-634c-4227-b064-12912a5bedf1","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":177,"statement_ids":[177]},"text/plain":["StatementMeta(, 8e0b499d-634c-4227-b064-12912a5bedf1, 177, Finished, Available, Finished)"]},"metadata":{},"output_type":"display_data"}],"source":["def mask_to_polygon(mask: np.ndarray) -> List[List[int]]:\n","    # Find contours in the binary mask\n","    contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","\n","    # Find the contour with the largest area\n","    largest_contour = max(contours, key=cv2.contourArea)\n","\n","    # Extract the vertices of the contour\n","    polygon = largest_contour.reshape(-1, 2).tolist()\n","\n","    return polygon\n","\n","def polygon_to_mask(polygon: List[Tuple[int, int]], image_shape: Tuple[int, int]) -> np.ndarray:\n","    \"\"\"\n","    Convert a polygon to a segmentation mask.\n","\n","    Args:\n","    - polygon (list): List of (x, y) coordinates representing the vertices of the polygon.\n","    - image_shape (tuple): Shape of the image (height, width) for the mask.\n","\n","    Returns:\n","    - np.ndarray: Segmentation mask with the polygon filled.\n","    \"\"\"\n","    # Create an empty mask\n","    mask = np.zeros(image_shape, dtype=np.uint8)\n","\n","    # Convert polygon to an array of points\n","    pts = np.array(polygon, dtype=np.int32)\n","\n","    # Fill the polygon with white color (255)\n","    cv2.fillPoly(mask, [pts], color=(255,))\n","\n","    return mask\n","# load the image from the url or local path\n","def load_image(image_str: str) -> Image.Image:\n","    if image_str.startswith(\"http\"):\n","        image = Image.open(requests.get(image_str, stream=True).raw).convert(\"RGB\")\n","    else:\n","        image = Image.open(image_str).convert(\"RGB\")\n","\n","    return image\n","# get the boxes from the detection results\n","def get_boxes(results: DetectionResult) -> List[List[List[float]]]:\n","    boxes = []\n","    for result in results:\n","        xyxy = result.box.xyxy\n","        boxes.append(xyxy)\n","\n","    return [boxes]\n","\n","def refine_masks(masks: torch.BoolTensor, polygon_refinement: bool = False) -> List[np.ndarray]:\n","    masks = masks.cpu().float()\n","    masks = masks.permute(0, 2, 3, 1)\n","    masks = masks.mean(axis=-1)\n","    masks = (masks > 0).int()\n","    masks = masks.numpy().astype(np.uint8)\n","    masks = list(masks)\n","\n","    if polygon_refinement:\n","        for idx, mask in enumerate(masks):\n","            shape = mask.shape\n","            polygon = mask_to_polygon(mask)\n","            mask = polygon_to_mask(polygon, shape)\n","            masks[idx] = mask\n","\n","    return masks"]},{"cell_type":"markdown","id":"ec50a5d2-c7a6-4cb3-8ada-cd200ac72131","metadata":{"id":"fErkFJkmlEMl"},"source":["## Grounded Segment Anything (SAM)\n","\n","Now it's time to define the Grounded SAM approach!\n","\n","The approach is very simple:\n","1. use Grounding DINO to detect a given set of texts in the image. The output is a set of bounding boxes.\n","2. prompt Segment Anything (SAM) with the bounding boxes, for which the model will output segmentation masks."]},{"cell_type":"code","execution_count":null,"id":"42f054d6-edf1-4c90-a181-9a0e04b5aaf9","metadata":{"id":"8YbAUF2ZkL7Y","microsoft":{"language":"python","language_group":"synapse_pyspark"}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-11-03T22:10:11.6596328Z","execution_start_time":"2024-11-03T22:10:11.4320402Z","livy_statement_state":"available","normalized_state":"finished","parent_msg_id":"693252ee-ea81-444f-b0c4-006868d12028","queued_time":"2024-11-03T22:10:06.3365628Z","session_id":"8e0b499d-634c-4227-b064-12912a5bedf1","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":178,"statement_ids":[178]},"text/plain":["StatementMeta(, 8e0b499d-634c-4227-b064-12912a5bedf1, 178, Finished, Available, Finished)"]},"metadata":{},"output_type":"display_data"}],"source":["def detect(\n","    image: Image.Image,\n","    labels: List[str],\n","    threshold: float = 0.3,\n","    detector_id: Optional[str] = None\n",") -> List[Dict[str, Any]]:\n","    \"\"\"\n","    Use Grounding DINO to detect a set of labels in an image in a zero-shot fashion.\n","    \"\"\"\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    detector_id = detector_id if detector_id is not None else \"IDEA-Research/grounding-dino-base\"\n","    processor = AutoProcessor.from_pretrained(detector_id)\n","    object_detector = AutoModelForZeroShotObjectDetection.from_pretrained(detector_id).to(device)\n","\n","    # add a dot to the end of each label\n","    labels = [label if label.endswith(\".\") else label+\".\" for label in labels]\n","    text = \" \".join(labels)\n","    # text = \"a cat. a remote control.\"\n","    # image_url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n","    # image = Image.open(requests.get(image_url, stream=True).raw)\n","    \n","    # process the image and text\n","    inputs = processor(images=image, text=text, return_tensors=\"pt\").to(device)\n","    with torch.no_grad():\n","        results = object_detector(**inputs)\n","    # post process the results\n","    results = processor.post_process_grounded_object_detection(\n","        results,\n","        inputs.input_ids,\n","        box_threshold=threshold,\n","        text_threshold=threshold,\n","        target_sizes=[image.size[::-1]]\n","    )\n","    # print(results)\n","    return results\n","\n","def segment(\n","    image: Image.Image,\n","    detection_results: List[Dict[str, Any]],\n","    polygon_refinement: bool = False,\n","    segmenter_id: Optional[str] = None\n",") -> List[DetectionResult]:\n","    \"\"\"\n","    Use Segment Anything (SAM) to generate masks given an image + a set of bounding boxes.\n","    \"\"\"\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    segmenter_id = segmenter_id if segmenter_id is not None else \"facebook/sam-vit-base\"\n","    boxes = get_boxes(detection_results)\n","    # print(boxes)\n","    segmentator = AutoModelForMaskedLM.from_pretrained(segmenter_id).to(device)\n","    processor = AutoProcessor.from_pretrained(segmenter_id)\n","    inputs = processor(images=image, input_boxes=boxes, return_tensors=\"pt\").to(device)\n","    # forward the model\n","    outputs = segmentator(**inputs)\n","    # post process the masks\n","    masks = processor.post_process_masks(\n","        masks=outputs.pred_masks,\n","        original_sizes=inputs.original_sizes,\n","        reshaped_input_sizes=inputs.reshaped_input_sizes\n","    )[0]\n","    # refine the masks\n","    masks = refine_masks(masks, polygon_refinement)\n","    # update the detection results\n","    for detection_result, mask in zip(detection_results, masks):\n","        detection_result.mask = mask\n","\n","    return detection_results\n","\n","def grounded_segmentation(\n","    image: Union[Image.Image, str],\n","    labels: List[str],\n","    threshold: float = 0.3,\n","    polygon_refinement: bool = False,\n","    detector_id: Optional[str] = None,\n","    segmenter_id: Optional[str] = None\n",") -> Tuple[np.ndarray, List[DetectionResult]]:\n","    \n","    image = load_image(image)\n","\n","    detections = detect(image, labels, threshold, detector_id)\n","    # detections = segment(image, detections, polygon_refinement, segmenter_id)\n","\n","    return np.array(image), detections"]},{"cell_type":"markdown","id":"43a67a5a-fd21-4da7-8a8d-ea88db39bce3","metadata":{"id":"Yo8cGKdxXWPR"},"source":["### Inference\n","\n","Let's showcase Grounded SAM on our favorite image: the cats image from the COCO dataset."]},{"cell_type":"code","execution_count":null,"id":"ed3c429c-a7a1-4e51-a91a-7836bfa544b8","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-11-03T22:10:15.3865112Z","execution_start_time":"2024-11-03T22:10:13.8636695Z","livy_statement_state":"available","normalized_state":"finished","parent_msg_id":"a8129067-5a19-4821-b643-0ff9ee932499","queued_time":"2024-11-03T22:10:06.8429693Z","session_id":"8e0b499d-634c-4227-b064-12912a5bedf1","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":182,"statement_ids":[182]},"text/plain":["StatementMeta(, 8e0b499d-634c-4227-b064-12912a5bedf1, 182, Finished, Available, Finished)"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>taskID</th>\n","      <th>picID</th>\n","      <th>curPicPath</th>\n","      <th>keywords</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>14</td>\n","      <td>4</td>\n","      <td>/lakehouse/default/Files/images/14/4_small.jpg</td>\n","      <td>sheep</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>14</td>\n","      <td>3</td>\n","      <td>/lakehouse/default/Files/images/14/3_small.jpg</td>\n","      <td>sheep</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>14</td>\n","      <td>2</td>\n","      <td>/lakehouse/default/Files/images/14/2_small.jpg</td>\n","      <td>sheep</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>14</td>\n","      <td>1</td>\n","      <td>/lakehouse/default/Files/images/14/1_small.jpg</td>\n","      <td>sheep</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>14</td>\n","      <td>0</td>\n","      <td>/lakehouse/default/Files/images/14/0_small.jpg</td>\n","      <td>sheep</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   taskID  picID                                      curPicPath keywords\n","0      14      4  /lakehouse/default/Files/images/14/4_small.jpg    sheep\n","1      14      3  /lakehouse/default/Files/images/14/3_small.jpg    sheep\n","2      14      2  /lakehouse/default/Files/images/14/2_small.jpg    sheep\n","3      14      1  /lakehouse/default/Files/images/14/1_small.jpg    sheep\n","4      14      0  /lakehouse/default/Files/images/14/0_small.jpg    sheep"]},"execution_count":538,"metadata":{},"output_type":"execute_result"}],"source":["# get image data\n","# taskID = 14\n","df = spark.sql(f\"SELECT * FROM InFreGen.pic WHERE state = '1' and taskID = {taskID} \")\n","data = df.select(\"taskID\", \"picID\", \"curPicPath\", \"keywords\").toPandas()\n","data"]},{"cell_type":"markdown","id":"877fd752-51c2-481e-b4fe-724df86b0b42","metadata":{"id":"-sJrT5xMf_Ad"},"source":["Let's visualize the results:"]},{"cell_type":"markdown","id":"471ba9ea-dfc4-4b2c-aa5d-30a60761f95f","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["# Cutting"]},{"cell_type":"code","execution_count":null,"id":"a459275d-c6be-4721-859a-f55b89904f15","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-11-03T22:16:32.93795Z","execution_start_time":"2024-11-03T22:15:56.2939933Z","livy_statement_state":"available","normalized_state":"finished","parent_msg_id":"2e1ae31f-62e1-4f18-ac0d-7afe02dca4d9","queued_time":"2024-11-03T22:15:55.8739Z","session_id":"8e0b499d-634c-4227-b064-12912a5bedf1","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":190,"statement_ids":[190]},"text/plain":["StatementMeta(, 8e0b499d-634c-4227-b064-12912a5bedf1, 190, Finished, Available, Finished)"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["/lakehouse/default/Files/images/14/4_small.jpg\n","['sheep']\n","tensor([0.9506])\n","/lakehouse/default/Files/images/14/4_small_cutted.jpg saved\n","/lakehouse/default/Files/images/14/3_small.jpg\n","['sheep']\n","tensor([0.7347, 0.7171])\n","/lakehouse/default/Files/images/14/3_small_cutted.jpg saved\n","/lakehouse/default/Files/images/14/2_small.jpg\n","['sheep']\n","/lakehouse/default/Files/images/14/1_small.jpg\n","['sheep']\n","tensor([0.8800])\n","/lakehouse/default/Files/images/14/1_small_cutted.jpg saved\n","/lakehouse/default/Files/images/14/0_small.jpg\n","['sheep']\n"]}],"source":["import json\n","from PIL import Image\n","\n","boundRes = {}\n","# threshold for the detection\n","threshold = 0.3\n","# detector id\n","detector_id = \"IDEA-Research/grounding-dino-base\"\n","segmenter_id = \"facebook/sam-vit-base\"\n","\n","for index, image in data.iterrows():\n","    \n","    picID = image['picID']\n","    image_path = image['curPicPath']\n","    if(image_path == \"\"):\n","        continue\n","    labels = [image['keywords']]\n","\n","    print(image_path)\n","    print(labels)\n","    try:\n","        # Perform grounded segmentation\n","        image_array, detections = grounded_segmentation(\n","            image=image_path,\n","            labels=labels,\n","            threshold=threshold,\n","            polygon_refinement=True,\n","            detector_id=detector_id,\n","            segmenter_id=segmenter_id\n","        )\n","    except:\n","        continue\n","\n","    # Get the detection boxes and labels\n","    boxes = detections[0][\"boxes\"].tolist()\n","    labels_list = detections[0][\"labels\"]\n","\n","    # Find the largest bounding box based on area\n","    max_box = None\n","    max_area = 0\n","\n","    print(detections[0][\"scores\"])\n","    for box in boxes:\n","        x_min, y_min, x_max, y_max = box\n","        area = (x_max - x_min) * (y_max - y_min)\n","        if area > max_area:\n","            max_area = area\n","            max_box = box\n","\n","    # If a bounding box was found, crop the image\n","    if max_box:\n","        x_min, y_min, x_max, y_max = max_box\n","        with Image.open(image_path) as img:\n","            cropped_img = img.crop((x_min, y_min, x_max, y_max))\n","\n","            # Save the cropped image with '_cutted' suffix\n","            output_path = image_path.replace(\".jpg\", \"_cutted.jpg\")\n","            print(output_path + \" saved\")\n","            cropped_img.save(output_path)\n","\n","        # Update boundRes with the bounding box info and output path\n","        boundRes[picID] = {\n","            \"boxes\": boxes,\n","            \"labels\": labels_list,\n","            \"largest_box\": max_box,\n","            \"cropped_image_path\": output_path\n","        }\n","\n","        spark.sql(f\"UPDATE InFreGen.pic SET state = '3', curPicPath = '{output_path}' WHERE taskID = '{taskID}' AND picID = '{picID}'\")\n","\n","\n","    # break\n","    # Optionally, convert the result to JSON format for saving or output\n","    # json_output = json.dumps(boundRes, indent=4)\n","    # print(json_output)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","include_colab_link":true,"provenance":[]},"dependencies":{"environment":{"environmentId":"6e51851b-5ce6-40c1-9f8e-a47237cbcdc7","workspaceId":"a136438a-0d8b-4308-9f2f-c74ea9668405"},"lakehouse":{"default_lakehouse":"89188849-0b09-4102-bcab-2cfcf5509b4b","default_lakehouse_name":"InFreGen","default_lakehouse_workspace_id":"a136438a-0d8b-4308-9f2f-c74ea9668405"}},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"widgets":{}},"nbformat":4,"nbformat_minor":5}
